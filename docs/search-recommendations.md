# ğŸ” Search + Recommendations

This section addresses the two critical elements of Catalog Excellence:

1. **Eliminating failed searches** (removing the weak link)
2. **Delivering great alternatives** (strengthening the strong link)

Together, these systems ensure that customers always find something relevantâ€”and often something better.

---

## 1. âŒ Weak Link: Eliminating Failed Searches

A failed search ("no results found" or irrelevant matches for specific terms) breaks user trustâ€”especially when the query contains **clear intent** or a **proper noun** (e.g., brand names like â€œNikeâ€).

### A. Systematic Failed Search Program

**Core Idea**: Create a living map of failed searches and systematically close them.

#### Steps:
1. Collect the **top 5,000 queries per category** over a trailing 30â€“60 day period.
2. Identify queries with:
   - Zero results
   - Poor engagement (e.g., high bounce, no clicks)
3. Classify root causes:
   - Catalog gap
   - Search ranking failure
   - Typo / synonym / alias mismatch

### âœ… Example
- Query: `boAt Nirvana Ion earbuds`
- Result: â€œNo results foundâ€ (due to missing listing)
- Fix: Identify from external sources â†’ Match and prioritize for onboarding

### B. Automated Discovery & Matching

- For each failed query:
  - Run it through the flattened external catalog
  - Attempt title/brand-based matching using fuzzy search (e.g., Levenshtein, BM25, or vector similarity)
  - Rank matched external items by relevance and popularity

### C. Resolution Pipeline

- **If item is on external catalog but missing internally** â†’ Candidate for onboarding
- **If item exists but doesnâ€™t show up in results** â†’ Search team reviews ranking + indexing
- **If itâ€™s a synonym/variant (e.g., â€œsneakersâ€ vs â€œrunning shoesâ€)** â†’ Update query understanding engine

### D. Retail & Ops Interface

- Dashboards showing top unresolved queries
- Review tools that show side-by-side:
  - Failed query
  - Suggested match
  - Current catalog coverage

### âœ… Example
- Query: `Khadi Neem Soap`
- Suggested match: `Khadi Natural Herbal Neem Soap 125g`
- Issue: Item exists but not indexed under the correct synonym

---

## 2. ğŸ’¡ Strong Link: Alternatives That Convert

When a user is still exploring, **surfacing the right alternatives** on search result pages (SERP) determines whether they stay, discover, and convert.

### A. Building the Recommendation Engine

**Goal**: Provide high-quality, intent-aligned alternatives before the user lands on a product page.

#### Input Signals:
- Brand proximity
- Price range
- Category overlap
- Glance views / impressions
- Add-to-cart and conversion data

#### Similarity Score Model:
A dynamic score for every product pair:
```
similarity_score = w1 * brand_similarity +
                   w2 * price_proximity +
                   w3 * shared_attributes +
                   w4 * engagement_overlap +
                   w5 * conversion_ratio_similarity
```

#### Predictive Edge Modeling:
To strengthen substitute recommendations:
- Train ML models (e.g., **XGBoost**) on product pair features:
  - Brand similarity, price difference, co-session frequency
  - Metadata alignment and conversion funnel similarity
- Output: High-confidence substitutes prioritized on SERP

### âœ… Example
- User searches: `wireless keyboard`
- Seen product: Logitech K230 Wireless Keyboard
- Alternatives shown:
  - Dell KM117 Wireless Keyboard (predicted substitute via XGBoost)
  - HP CS10 Combo

### B. Graph-Integrated Recommendations

Plug similarity scores and ML-predicted substitutes into the **product graph**:
- Use edges to recommend variants, substitutes
- Dynamically update edge weights based on real-time user signals

### âœ… Example
- Node A: Apple iPad Air 5th Gen
- Node B: Samsung Galaxy Tab S8
- Edge Type: Substitute (ML-predicted and reinforced by user co-view)

### C. Learning from Outcomes

- Measure **win ratio**: how often a recommended alternative is chosen over the original path
- Train the model to optimize for win ratio over clicks
- Include diversity constraints to avoid repetition

---

## 3. ğŸ” Feedback Loops

- Capture false positives (bad recos) and missed intents via:
  - User feedback
  - Retail input
  - A/B tests

### âœ… Example
- Reco shown: â€œApple Watch Bandâ€ as an alternative to â€œSamsung Galaxy Watchâ€
- Feedback: Retail marks this as irrelevant â†’ Down-rank edge in graph

- Use this data to:
  - Adjust weights
  - Refine edge generation in product graph
  - Improve ranking logic

---

## âš ï¸ Risks & Mitigations

| Risk                                 | Mitigation Approach                                        |
|--------------------------------------|-------------------------------------------------------------|
| Recos too similar to original        | Inject diversity constraints, control for novelty bias     |
| Good products buried in search       | Combine catalog + search telemetry to surface under-indexed SKUs |
| Query intent misclassified           | Use supervised intent modeling + synonym dictionaries       |
| Overload of suggestions              | Limit visible alternatives to high-confidence, high-impact ones |

---

## âœ… Outcome

A powerful feedback-driven loop between **Search**, **Recommendations**, and **Catalog** that:
- Closes high-signal gaps
- Surfaces meaningful alternatives (driven by predictive models)
- Keeps users engaged and discovering

> â€œSearch is not a box. Itâ€™s a dialogue. Recos arenâ€™t decoration. Theyâ€™re decision support.â€
